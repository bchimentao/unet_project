{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1834160,"sourceType":"datasetVersion","datasetId":333968}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Aerial Image Segmentation using U-Net\n\nThis Jupyter Notebook aims to segment aerial images using an U-Net model.\n\nThe images come from Kaggle's \"Aerial Semantic Segmentation Drone Dataset\" database.","metadata":{}},{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nfrom tqdm import tqdm\nimport random\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import OneHotMeanIoU, IoU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, BatchNormalization\nfrom tensorflow.keras.layers import Activation, MaxPool2D, Concatenate","metadata":{"execution":{"iopub.status.busy":"2024-05-12T01:58:43.934683Z","iopub.execute_input":"2024-05-12T01:58:43.935319Z","iopub.status.idle":"2024-05-12T01:59:04.971503Z","shell.execute_reply.started":"2024-05-12T01:58:43.935288Z","shell.execute_reply":"2024-05-12T01:59:04.970442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking if the code is running on the graphics processing unit\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{"execution":{"iopub.status.busy":"2024-05-12T01:59:04.973565Z","iopub.execute_input":"2024-05-12T01:59:04.974092Z","iopub.status.idle":"2024-05-12T01:59:05.482493Z","shell.execute_reply.started":"2024-05-12T01:59:04.974065Z","shell.execute_reply":"2024-05-12T01:59:05.481433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Necessary informations","metadata":{}},{"cell_type":"code","source":"# Insert the paths of the original images and the masks\noriginal_images_path = '/kaggle/input/semantic-drone-dataset/dataset/semantic_drone_dataset/original_images/'\nlabel_images_path = '/kaggle/input/semantic-drone-dataset/dataset/semantic_drone_dataset/label_images_semantic/'\n\n# Insert the CSV file containing the colors (rgb.csv)\nclass_dict_csv = '/kaggle/input/semantic-drone-dataset/class_dict_seg.csv'\n\n# Insert the number of epochs the model will run\nepochs = 100\n\n# Insert the batch size\nbatch_size = 8","metadata":{"execution":{"iopub.status.busy":"2024-05-12T01:59:05.484017Z","iopub.execute_input":"2024-05-12T01:59:05.484457Z","iopub.status.idle":"2024-05-12T01:59:05.512443Z","shell.execute_reply.started":"2024-05-12T01:59:05.484418Z","shell.execute_reply":"2024-05-12T01:59:05.511435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Image width and height (set to 512x512 as specified in the header)\nimg_width = 512\nimg_height = 512\n\n# Number of channels\nimg_channels = 3","metadata":{"execution":{"iopub.status.busy":"2024-05-12T01:59:05.513570Z","iopub.execute_input":"2024-05-12T01:59:05.513873Z","iopub.status.idle":"2024-05-12T01:59:05.522190Z","shell.execute_reply.started":"2024-05-12T01:59:05.513849Z","shell.execute_reply":"2024-05-12T01:59:05.521489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading Images","metadata":{}},{"cell_type":"code","source":"# A function to count how many times the classes appears in the dataset\ndef count_class(folder, class_csv):\n    count_color = {}\n\n    # Loop through all images in the folder\n    for file in os.listdir(folder):\n        if file.endswith(\".png\") or file.endswith(\".jpg\") or file.endswith(\".jpeg\"):\n            path_img = os.path.join(folder, file)\n\n            # Load the image in grayscale\n            img = cv2.imread(path_img, cv2.IMREAD_GRAYSCALE)\n\n            # Get unique colors from the image\n            unique_colors = np.unique(img)\n\n            # Update color count\n            for color in unique_colors:\n                count_color[color] = count_color.get(color, 0) + 1\n\n    # Reading the csv with the classes\n    classes = pd.read_csv(class_csv)\n    \n    # Map color values to classes according to the provided list\n    map_class = {}\n    for idx, row in classes.iterrows():\n        map_class[idx] = row['name']\n\n    # Create DataFrame from the count dictionary\n    df_count = pd.DataFrame(list(count_color.items()), columns=['Color', 'Count'])\n\n    # Sort the DataFrame in ascending order of \"Color\" column values\n    df_count = df_count.sort_values(by='Color')\n\n    # Replace color name with class\n    df_count['Class'] = df_count['Color'].map(map_class)\n\n    # Discard the \"Color\" column and reorder the columns\n    df_count = df_count[['Class', 'Count']]\n\n    return df_count\n\n# how many time each class appear in the dataset\nprint(count_class(label_images_path, class_dict_csv).to_string(index=False))","metadata":{"execution":{"iopub.status.busy":"2024-05-12T01:59:05.524499Z","iopub.execute_input":"2024-05-12T01:59:05.524821Z","iopub.status.idle":"2024-05-12T02:03:01.830272Z","shell.execute_reply.started":"2024-05-12T01:59:05.524796Z","shell.execute_reply":"2024-05-12T02:03:01.829287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to get the images and masks extensions\ndef file_extension(path):    \n    name = path.split(\"/\")[-1].split(\".\")\n    file_extn = name[1] # file extension\n\n    return file_extn\n\n\n# Function created to generate a path for a random image for future plotting\ndef random_file(path):\n    # Lists all files in a directory\n    files = os.listdir(path)\n    \n    # Filters only files (removes subdirectories)\n    files = [file for file in files if os.path.isfile(os.path.join(path, file))]\n    \n    # Returns a random file\n    return random.choice(files)\n\n\n# the images extension\nimg_ext = file_extension(random_file(original_images_path))\n\n# the masks extension\nmask_ext = file_extension(random_file(label_images_path))","metadata":{"execution":{"iopub.status.busy":"2024-05-12T02:03:01.831675Z","iopub.execute_input":"2024-05-12T02:03:01.832035Z","iopub.status.idle":"2024-05-12T02:03:02.318168Z","shell.execute_reply.started":"2024-05-12T02:03:01.832003Z","shell.execute_reply":"2024-05-12T02:03:02.317382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A random image and mask will be displayed\nrandom_img = random_file(original_images_path)\n\nrandom_mask = f'{random_img.split(\".\")[0]}.{mask_ext}'\n\nprint(cv2.imread(random_img))\n\nfig = plt.figure(figsize=(12,7))\n\nfig.add_subplot(1,2,1) \nplt.imshow(cv2.cvtColor(cv2.imread(original_images_path + random_img), cv2.COLOR_BGR2RGB))\nplt.axis(\"off\")\nplt.title(\"Original Image\")\n\nfig.add_subplot(1,2,2) \nplt.imshow(cv2.imread(label_images_path + random_mask, cv2.IMREAD_GRAYSCALE))\nplt.axis(\"off\")\nplt.title(\"Label Image\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T02:03:02.319244Z","iopub.execute_input":"2024-05-12T02:03:02.319523Z","iopub.status.idle":"2024-05-12T02:03:07.344249Z","shell.execute_reply.started":"2024-05-12T02:03:02.319500Z","shell.execute_reply":"2024-05-12T02:03:07.343327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to create a DataFrame from file names in a directory\ndef create_dataframe(path):\n    # List to store file names\n    name = []\n    \n    # Iterates over the files in the specified directory\n    for dirname, _, filenames in os.walk(path):\n        for filename in filenames:\n            # Adds the file name (without extension) to the list\n            name.append(filename.split('.')[0])\n    \n    # Creates a DataFrame with the file names as a column named 'id'\n    return pd.DataFrame({'id': name}, index=np.arange(0, len(name)))\n\n\n# Creates a DataFrame for the original images\ndf_images = create_dataframe(original_images_path)\n\n# Creates a DataFrame for the masks\ndf_masks = create_dataframe(label_images_path)\n\n# Prints the total number of images found\nprint('Total Images: ', len(df_images))","metadata":{"execution":{"iopub.status.busy":"2024-05-12T02:03:07.345532Z","iopub.execute_input":"2024-05-12T02:03:07.345846Z","iopub.status.idle":"2024-05-12T02:03:07.362589Z","shell.execute_reply.started":"2024-05-12T02:03:07.345820Z","shell.execute_reply":"2024-05-12T02:03:07.361725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Division of the DataFrame into training, testing, and validation sets\n# Uses the train_test_split function from scikit-learn to split the data\n# X_trainval contains a fraction of the data for training and validation, and X_test contains the rest for testing\nX_trainval, X_test = train_test_split(df_images['id'], test_size=0.1, random_state=20)\n\n# Divides X_trainval again into training and validation sets\n# X_train contains the majority of the data for training and X_val contains a fraction for validation\nX_train, X_val = train_test_split(X_trainval, test_size=0.2, random_state=34)\n\n# Prints the size of each set\nprint(f\"Train Size : {len(X_train)} images\")\nprint(f\"Val Size   :  {len(X_val)} images\")\nprint(f\"Test Size  :  {len(X_test)} images\")\n\n# Defines the labels for the training, testing, and validation sets\ny_train = X_train\ny_test = X_test\ny_val = X_val\n\n# Constructs lists of file paths for images and masks in each set\nimg_train = [os.path.join(original_images_path, f\"{name}.{img_ext}\") for name in X_train]\nmask_train = [os.path.join(label_images_path, f\"{name}.{mask_ext}\") for name in y_train]\nimg_val = [os.path.join(original_images_path, f\"{name}.{img_ext}\") for name in X_val]\nmask_val = [os.path.join(label_images_path, f\"{name}.{mask_ext}\") for name in y_val]\nimg_test = [os.path.join(original_images_path, f\"{name}.{img_ext}\") for name in X_test]\nmask_test = [os.path.join(label_images_path, f\"{name}.{mask_ext}\") for name in y_test]\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T02:03:07.363850Z","iopub.execute_input":"2024-05-12T02:03:07.364460Z","iopub.status.idle":"2024-05-12T02:03:07.387768Z","shell.execute_reply.started":"2024-05-12T02:03:07.364428Z","shell.execute_reply":"2024-05-12T02:03:07.386810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DataFrame generated from the CSV file of colors\npd.read_csv(class_dict_csv)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T02:03:07.389102Z","iopub.execute_input":"2024-05-12T02:03:07.389381Z","iopub.status.idle":"2024-05-12T02:03:07.418271Z","shell.execute_reply.started":"2024-05-12T02:03:07.389359Z","shell.execute_reply":"2024-05-12T02:03:07.417340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of level 2 classes plus the unlabeled class\nnum_classes = len(pd.read_csv(class_dict_csv))\nnum_classes","metadata":{"execution":{"iopub.status.busy":"2024-05-12T02:03:07.419278Z","iopub.execute_input":"2024-05-12T02:03:07.419546Z","iopub.status.idle":"2024-05-12T02:03:07.428523Z","shell.execute_reply.started":"2024-05-12T02:03:07.419523Z","shell.execute_reply":"2024-05-12T02:03:07.427656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building Neural Network","metadata":{}},{"cell_type":"code","source":"# Definition of a convolutional block with two convolutional layers\n# This block is a basic operation in convolutional neural networks to extract features from an image\ndef conv_block(input, num_filters):\n  # First convolutional layer\n  x = Conv2D(num_filters, 3, padding='same')(input)  # Applies convolution with num_filters filters and a filter size of 3x3\n  x = BatchNormalization()(x)  # Normalizes the output channel values\n  x = Activation('relu')(x)    # Applies ReLU (Rectified Linear Unit) activation function\n\n  # Second convolutional layer\n  x = Conv2D(num_filters, 3, padding='same')(x)  # Another convolution with num_filters filters and a filter size of 3x3\n  x = BatchNormalization()(x)  # Normalization\n  x = Activation('relu')(x)    # ReLU\n\n  return x  # Returns the output of the convolutional block\n\n\n# Definition of an encoder block in a U-Net architecture\n# This block consists of a convolutional layer followed by a max pooling layer\ndef encoder_block(input, num_filters):\n  # Calling the conv_block function to create a convolutional layer\n  x = conv_block(input, num_filters)\n  \n  # Max pooling layer to reduce the dimensionality of the input by half\n  p = MaxPool2D((2,2))(x)  # Using max pooling with a filter size of 2x2 and default stride\n  \n  return x, p  # Returns the output of the convolutional block (x) and the output of max pooling (p)\n\n\n# Definition of a decoder block in a U-Net architecture\n# This block consists of a transposed convolutional layer followed by concatenation with encoder features and a convolutional layer\ndef decoder_block(input, skip_features, num_filters):\n  # Transposed convolutional layer to increase the spatial resolution of the input\n  x = Conv2DTranspose(num_filters, (2,2), strides=2, padding='same')(input)  # Using transposed convolution with a filter size of 2x2 and stride 2\n  \n  # Concatenation with encoder features\n  x = Concatenate()([x, skip_features])  # Concatenates the output of the transposed convolution with the encoder features\n  \n  # Calling the conv_block function to create a convolutional layer\n  x = conv_block(x, num_filters)\n  \n  return x  # Returns the output of the block\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T02:03:07.432033Z","iopub.execute_input":"2024-05-12T02:03:07.432327Z","iopub.status.idle":"2024-05-12T02:03:07.440939Z","shell.execute_reply.started":"2024-05-12T02:03:07.432291Z","shell.execute_reply":"2024-05-12T02:03:07.440127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Definition of a U-Net network architecture\n# This architecture consists of encoder and decoder blocks, which are used for image segmentation\ndef unet_model(input_shape):\n  # Input layer of the network\n  inputs = Input(input_shape)\n\n  # Encoder: convolutional blocks to extract features from the input images\n  s1, p1 = encoder_block(inputs, 64)    # First encoder block\n  s2, p2 = encoder_block(p1, 128)       # Second encoder block\n  s3, p3 = encoder_block(p2, 256)       # Third encoder block\n  s4, p4 = encoder_block(p3, 512)       # Fourth encoder block\n\n  # Bottleneck: convolutional block to process features at a reduced resolution\n  b1 = conv_block(p4, 1024)\n\n  # Decoder: blocks to perform upsampling and generate the output mask\n  d1 = decoder_block(b1, s4, 512)  # First decoder block\n  d2 = decoder_block(d1, s3, 256)  # Second decoder block\n  d3 = decoder_block(d2, s2, 128)  # Third decoder block\n  d4 = decoder_block(d3, s1, 64)   # Fourth decoder block\n\n  # Output layer: convolution to generate the final segmentation mask\n  outputs = Conv2D(num_classes, 1, padding='same', activation='softmax')(d4)\n\n  # Model creation\n  model = Model(inputs, outputs, name='UNet')\n  return model","metadata":{"execution":{"iopub.status.busy":"2024-05-12T02:03:07.442143Z","iopub.execute_input":"2024-05-12T02:03:07.442431Z","iopub.status.idle":"2024-05-12T02:03:07.458021Z","shell.execute_reply.started":"2024-05-12T02:03:07.442408Z","shell.execute_reply":"2024-05-12T02:03:07.457075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiation of the U-Net model with specified input dimensions\nmodel = unet_model((img_height, img_width, img_channels))\n\n# Compilation of the model with Adam optimizer, categorical crossentropy loss function, and accuracy and IoU metrics\nmodel.compile(optimizer=Adam(learning_rate=1e-4),\n              loss='categorical_crossentropy',\n              metrics=['accuracy',\n                       OneHotMeanIoU(num_classes=num_classes)])\n\n# Prints a summary of the model, showing the network architecture and the number of parameters\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T02:03:07.462093Z","iopub.execute_input":"2024-05-12T02:03:07.462348Z","iopub.status.idle":"2024-05-12T02:03:08.532903Z","shell.execute_reply.started":"2024-05-12T02:03:07.462328Z","shell.execute_reply":"2024-05-12T02:03:08.531985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Callbacks","metadata":{}},{"cell_type":"code","source":"# Function that creates a directory if it doesn't exist\ndef create_dir(path):\n    # Checks if the directory does not exist\n    if not os.path.exists(path):\n        # Creates the directory\n        os.makedirs(path)\n\n        \n# Creating the folder where trained model weights and final model will be saved\ncreate_dir('data/models/')  # Creates the 'models/' directory if it doesn't exist\n\n# Defines the name of the folder where model weights will be saved\nfolderName = 'data/models/'\n\n# Defines the path to save the model weights\ncheckpoint_path = folderName\ncheckpoint_path += \"/cp-{epoch:04d}.weights.h5\"  # The format of the weight files' name is 'cp-XXXX.weights.h5', where XXXX is the epoch number\n\n# Calculates the number of batches per epoch\nn_batches = math.ceil(len(X_train) / batch_size)\n\n# Creates a callback that saves the model weights periodically during training\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,  # Path to save the weights\n                                                 save_weights_only=True,  # Save only the model weights\n                                                 save_freq=25*n_batches,  # Frequency of saving weights (every 25 batches per epoch)\n                                                 verbose=1)  # Display information about saving\n\n# Saves the initial model weights using the format defined in 'checkpoint_path'\nmodel.save_weights(checkpoint_path.format(epoch=0))  # Saves the weights after epoch 0","metadata":{"execution":{"iopub.status.busy":"2024-05-12T02:03:08.534094Z","iopub.execute_input":"2024-05-12T02:03:08.534393Z","iopub.status.idle":"2024-05-12T02:03:08.868282Z","shell.execute_reply.started":"2024-05-12T02:03:08.534367Z","shell.execute_reply":"2024-05-12T02:03:08.867495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"def read_image(x):\n    x = cv2.imread(x, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (img_width, img_height))\n    x = x/255.0\n    x = x.astype(np.float32)\n    return x\n\n\ndef read_mask(x):\n    x = cv2.imread(x, cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x, (img_width, img_height))\n    x = x.astype(np.int32)\n    return x\n\n# Function to create a TensorFlow dataset\ndef tf_dataset(x, y, batch=batch_size):\n    # Creates a dataset from tensors (image and mask file paths)\n    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n    # Shuffles the dataset\n    dataset = dataset.shuffle(buffer_size=500)\n    # Applies the preprocessing function\n    dataset = dataset.map(preprocess)\n    # Batches the data\n    dataset = dataset.batch(batch)\n    # Repeats the dataset indefinitely\n    dataset = dataset.repeat()\n    # Preloads batches to speed up training\n    dataset = dataset.prefetch(2)\n    return dataset\n\n# Preprocessing function to normalize and convert images and masks\ndef preprocess(x, y):\n    # Internal function to decode file paths\n    def f(x, y):\n        x = x.decode()\n        y = y.decode()\n        # Reads the image and mask\n        image = read_image(x)\n        mask = read_mask(y)\n        return image, mask\n    \n    # Applies function f using numpy_function\n    image, mask = tf.numpy_function(f, [x, y], [tf.float32, tf.int32])\n    # Converts the mask into one-hot encoding\n    mask = tf.one_hot(mask, num_classes, dtype=tf.int32)\n    # Sets the shapes of the images and masks\n    image.set_shape([img_height, img_width, 3])    # In images, number of channels = 3\n    mask.set_shape([img_height, img_width, num_classes])    # In masks, number of channels = number of classes\n    return image, mask","metadata":{"execution":{"iopub.status.busy":"2024-05-12T02:03:08.869391Z","iopub.execute_input":"2024-05-12T02:03:08.869702Z","iopub.status.idle":"2024-05-12T02:03:08.880354Z","shell.execute_reply.started":"2024-05-12T02:03:08.869675Z","shell.execute_reply":"2024-05-12T02:03:08.879540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the number of training and validation steps\ntrain_steps = len(img_train) // batch_size\nvalid_steps = len(img_val) // batch_size\n\n# Create training and validation datasets\ntrain_dataset = tf_dataset(img_train, mask_train, batch=batch_size)\nvalid_dataset = tf_dataset(img_val, mask_val, batch=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T02:03:08.881456Z","iopub.execute_input":"2024-05-12T02:03:08.881728Z","iopub.status.idle":"2024-05-12T02:03:09.056540Z","shell.execute_reply.started":"2024-05-12T02:03:08.881704Z","shell.execute_reply":"2024-05-12T02:03:09.055634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(train_dataset,\n                    steps_per_epoch=train_steps, \n                    epochs=epochs, \n                    validation_data=valid_dataset, validation_steps=valid_steps,\n                    callbacks=[cp_callback]\n                    )","metadata":{"execution":{"iopub.status.busy":"2024-05-12T02:03:09.057944Z","iopub.execute_input":"2024-05-12T02:03:09.058624Z","iopub.status.idle":"2024-05-12T07:01:01.061359Z","shell.execute_reply.started":"2024-05-12T02:03:09.058571Z","shell.execute_reply":"2024-05-12T07:01:01.060163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saves the model in the models folder\nmodel.save(f'{folderName}/model.keras')","metadata":{"execution":{"iopub.status.busy":"2024-05-12T07:01:01.063038Z","iopub.execute_input":"2024-05-12T07:01:01.063315Z","iopub.status.idle":"2024-05-12T07:01:02.410718Z","shell.execute_reply.started":"2024-05-12T07:01:01.063291Z","shell.execute_reply":"2024-05-12T07:01:02.409822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function created to display the accuracy, loss, and mean IoU graphs for training\ndef plot_graphs(history):\n  fig = plt.gcf()\n  fig.set_size_inches(16,4)\n  plt.subplot(1,3,1)\n  plt.plot(history.history['accuracy'], 'red', label='Training Accuracy')\n  plt.plot(history.history['val_accuracy'], 'blue', label='Validation Accuracy')\n  plt.legend()\n  plt.title('Accuracy')\n\n  plt.subplot(1,3,2)\n  plt.plot(history.history['loss'], 'red', label='Training Loss')\n  plt.plot(history.history['val_loss'], 'blue', label='Validation Loss')\n  plt.legend()\n  plt.title('Loss')\n\n  plt.subplot(1,3,3)\n  plt.plot(history.history['one_hot_mean_io_u'], 'red', label='Training OneHotMeanIoU')\n  plt.plot(history.history['val_one_hot_mean_io_u'], 'blue', label='Validation OneHotMeanIoU')\n  plt.legend()\n  plt.title('OneHotMeanIoU')\n  plt.show()\n\n\n# Plotting the graphs\nplot_graphs(history)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T07:01:02.412036Z","iopub.execute_input":"2024-05-12T07:01:02.412354Z","iopub.status.idle":"2024-05-12T07:01:03.122242Z","shell.execute_reply.started":"2024-05-12T07:01:02.412327Z","shell.execute_reply":"2024-05-12T07:01:03.121257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tests","metadata":{}},{"cell_type":"code","source":"# loading the trained model\nloaded_model = tf.keras.models.load_model(f'{folderName}/model.keras')","metadata":{"execution":{"iopub.status.busy":"2024-05-12T07:01:03.123405Z","iopub.execute_input":"2024-05-12T07:01:03.123685Z","iopub.status.idle":"2024-05-12T07:01:13.361897Z","shell.execute_reply.started":"2024-05-12T07:01:03.123661Z","shell.execute_reply":"2024-05-12T07:01:13.360775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creation of a folder for the prediction results\npredict_path = 'data/results1/'\ncreate_dir(predict_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T07:01:13.363250Z","iopub.execute_input":"2024-05-12T07:01:13.363546Z","iopub.status.idle":"2024-05-12T07:01:13.368526Z","shell.execute_reply.started":"2024-05-12T07:01:13.363521Z","shell.execute_reply":"2024-05-12T07:01:13.367533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x, y in tqdm(zip(img_test, mask_test), total=len(img_test)):\n    name = y.split(\"/\")[-1]\n    \n    ## Read the original image\n    x = cv2.imread(x, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (img_width, img_height))\n    x = x/255.0\n    x = x.astype(np.float32)\n\n    ## Read the mask\n    y = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n    y = cv2.resize(y, (img_width, img_height))\n    \n    # Expand the dimensions of the mask to add a channel, making it (256,256,1).\n    y = np.expand_dims(y, axis=-1)\n    \n    # Normalize the mask values to be in the range [0, 255], according to the number of classes.\n    y = y.astype(np.int32)\n\n    # Expand the mask to have 3 channels, to match the image dimension.\n    y = np.concatenate([y, y, y], axis=2)\n    \n    ## Prediction\n    # Make the mask prediction using the loaded model, providing the input image.\n    p = loaded_model.predict(np.expand_dims(x, axis=0))[0]\n\n    # Get the class with the highest probability for each pixel of the predicted mask.\n    p = np.argmax(p, axis=-1)\n    \n    # Expand the dimensions of the predicted mask to add a channel.\n    p = np.expand_dims(p, axis=-1)  \n    \n    # Normalize the values of the predicted mask to be in the range [0, 255], according to the number of classes.\n    p = p.astype(np.int32)\n\n    # Expand the predicted mask to have 3 channels, to match the image dimension.\n    p = np.concatenate([p, p, p], axis=2)\n    \n    # Save the predicted mask as an image using OpenCV.\n    cv2.imwrite(predict_path + name, p)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T07:01:13.369851Z","iopub.execute_input":"2024-05-12T07:01:13.370222Z","iopub.status.idle":"2024-05-12T07:01:59.379866Z","shell.execute_reply.started":"2024-05-12T07:01:13.370184Z","shell.execute_reply":"2024-05-12T07:01:59.378860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculates the IoU for each class and returns a DataFrame\ndef iou_result(mask, predict, class_csv):\n    # Read the mask image and predicted image\n    mask_ = read_mask(mask)\n    predict_ = read_mask(predict)\n\n    # Reading the csv with the classes\n    classes = pd.read_csv(class_csv)\n\n    # Count the number of classes\n    n_classes = len(classes)\n\n    # Dictionary to store IoU values for each class\n    iou_dict = {}\n\n    # Iterate through unique predicted class IDs\n    for id in np.unique(predict_):\n        # Check if the class ID exists in ground truth classes\n        if id in np.unique(mask_):\n            # Calculate IoU for the current class ID\n            iou_img = IoU(num_classes=n_classes, target_class_ids=[id])    \n            iou_img.update_state(mask_, predict_)\n            iou_value = iou_img.result().numpy()\n            iou_dict[id] = iou_value\n        \n    # Map color values to classes according to the provided list\n    map_class = {}\n    for idx, row in classes.iterrows():\n        map_class[idx] = row['name']\n\n    # Create DataFrame from the count dictionary\n    iou_df = pd.DataFrame(list(iou_dict.items()), columns=['Num Class', 'IoU'])\n\n    # Replace color name with class\n    iou_df['Class'] = iou_df['Num Class'].map(map_class)\n\n    # Discard the \"Num Class\" column and reorder the columns\n    iou_df = iou_df[['Class', 'IoU']]\n\n    return iou_df","metadata":{"execution":{"iopub.status.busy":"2024-05-12T07:01:59.381083Z","iopub.execute_input":"2024-05-12T07:01:59.381374Z","iopub.status.idle":"2024-05-12T07:01:59.389705Z","shell.execute_reply.started":"2024-05-12T07:01:59.381349Z","shell.execute_reply":"2024-05-12T07:01:59.388779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This loop opens the corresponding images, masks, and predictions\nfor pred in os.listdir(predict_path):\n    mask = label_images_path + pred\n    img = original_images_path + pred.split('.')[-2] + '.jpg'\n    predict = predict_path + pred\n\n    x = cv2.imread(img, cv2.IMREAD_COLOR)\n    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n    x = cv2.resize(x, (img_width, img_height))\n\n    y = cv2.imread(mask, cv2.IMREAD_GRAYSCALE)\n    y = cv2.resize(y, (img_width, img_height))\n\n    p = cv2.imread(predict, cv2.IMREAD_GRAYSCALE)\n\n    print(f'{pred} - IoU results')\n    print(iou_result(mask, predict, class_dict_csv).to_string(index=False))\n    \n    # Plot the images\n    fig, axs = plt.subplots(1, 3, figsize=(20, 20), constrained_layout=True)\n\n    axs[0].imshow(x, interpolation='nearest')\n    axs[0].set_title(pred.split('.')[-2] + '.jpg')\n    axs[0].grid(False)\n\n    axs[1].imshow(y, interpolation='nearest')\n    axs[1].set_title('mask')\n    axs[1].grid(False)\n\n    axs[2].imshow(p, interpolation='nearest')\n    axs[2].set_title('prediction')\n    axs[2].grid(False)\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T07:01:59.390783Z","iopub.execute_input":"2024-05-12T07:01:59.391039Z","iopub.status.idle":"2024-05-12T07:03:20.642776Z","shell.execute_reply.started":"2024-05-12T07:01:59.391017Z","shell.execute_reply":"2024-05-12T07:03:20.641871Z"},"trusted":true},"execution_count":null,"outputs":[]}]}